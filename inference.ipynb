{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bf458f",
   "metadata": {},
   "source": [
    "# EyesMol model의 inference 코드",
    "( 0.3 epochs checkpoint 테스트 - 수정예정 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940c9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/workspace/EyesMolProject/Qwen2-VL-Finetune')\n",
    "from src.utils import load_pretrained_model, get_model_name_from_path, disable_torch_init\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7af45823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-28 08:16:00,785] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading Qwen2-VL from base model...\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.19s/it]\n",
      "Loading additional Qwen2-VL weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model Loaded!!!\n"
     ]
    }
   ],
   "source": [
    "# LoRA 모델 병합\n",
    "lora_path = '/workspace/checkpoint-18000'\n",
    "save_path = '/workspace/chebkpoint_merge'  # 기존에 있는 폴더 사용\n",
    "model_base = '/workspace/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "# f-string을 사용하여 변수를 명령어에 전달\n",
    "!python /workspace/EyesMolProject/Qwen2-VL-Finetune/src/merge_lora_weights.py --model-path {lora_path} --model-base {model_base} --save-model-path {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03843266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 경로: /workspace/chebkpoint_merge\n",
      "베이스 모델: /workspace/Qwen2.5-VL-3B-Instruct\n",
      "디바이스: cuda\n"
     ]
    }
   ],
   "source": [
    "# 모델 경로 설정\n",
    "MODEL_PATH = save_path\n",
    "MODEL_BASE = model_base  # 이제 위 셀에서 정의된 변수 사용 가능\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 생성 파라미터\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0\n",
    "REPETITION_PENALTY = 1.0\n",
    "\n",
    "print(f\"모델 경로: {MODEL_PATH}\")\n",
    "print(f\"베이스 모델: {MODEL_BASE}\")\n",
    "print(f\"디바이스: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e01604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /workspace/chebkpoint_merge as a standard model. Adapter files were not found, so it can't be merged\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837dbe659687423ebbb759baa76df82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "disable_torch_init()\n",
    "model_name = get_model_name_from_path(MODEL_PATH)\n",
    "processor, model = load_pretrained_model(\n",
    "    model_base=MODEL_BASE,\n",
    "    model_path=MODEL_PATH,\n",
    "    device_map=DEVICE,\n",
    "    model_name=model_name,\n",
    "    load_4bit=False,\n",
    "    load_8bit=False,\n",
    "    device=DEVICE,\n",
    "    use_flash_attn=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "print(\"모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b6613fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, image_path=None):\n",
    "    start_time = time.time()\n",
    "    conversation = []\n",
    "    user_content = []\n",
    "\n",
    "    if image_path:\n",
    "        user_content.append({\"type\": \"image\", \"image\": image_path})\n",
    "    if question:\n",
    "        user_content.append({\"type\": \"text\", \"text\": question})\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_content})\n",
    "    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[prompt], \n",
    "        images=image_inputs, \n",
    "        videos=video_inputs, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"do_sample\": True if TEMPERATURE > 0 else False,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY,\n",
    "        \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, **generation_args)\n",
    "        \n",
    "    input_token_len = inputs['input_ids'].shape[1]\n",
    "    response_ids = output_ids[:, input_token_len:]\n",
    "    response = processor.tokenizer.decode(response_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    inference_time = time.time() - start_time\n",
    "    return response, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "119c9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 \n",
      " 응답 : 이 분자체는 1,2,3,4,5,6-hexahydro-1,2,3-trimethyl-1H-inden-1-ol입니다. 이는 1,2,3,4,5,6-hexahydro-1H-inden-1-ol의 산화산물로, 1,2,3,4,5,6-hexahydro-1H-inden-1-ol의 산화산물로 간주됩니다.\n",
      "소요시간 (초) : 6.679670333862305\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/workspace/8.png\"\n",
    "question_with_image = \"이 분자에 대해서 아주 자세하게 설명해줘\"\n",
    "response, inference_time = generate_response(question_with_image, image_path=image_path)\n",
    "print(f'모델 \\n 응답 : {response}')\n",
    "print(f'소요시간 (초) : {inference_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bd692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c63349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48b8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0563401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
